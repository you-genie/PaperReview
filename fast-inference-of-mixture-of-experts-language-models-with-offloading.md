---
description: https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language
---

# Fast Inference of Mixture-of-Experts Language Models with Offloading

## Description

### Summary

* Mixtral-8x7B가 최종 모델임
* MoE는 좋은 방식이지만 모델이 너무 크다. 게이트 감안해도 크다.

\---

* link: [https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language](https://paperswithcode.com/paper/fast-inference-of-mixture-of-experts-language)

## Introduction

MoE는 좋은 방식이지만 모델이 너무 크다. 게이트 감안해도 크다.

어떻게 하면 efficient하게 MoE를 실행시킬 수 있을까? 에 대한 해답 제시.

다음 두 가지 질문에 대한 답변을 제시하고 있다.

* MoE가 어떻게 각 experts를 쓸지 결정하는가?
* 어떻게 MoE를 더 efficient하게 쓸 수 있을까?

## Method

## Experiments

## Discussion
